{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> $\\color{red}{\\text{Text Analysis-1}}$ </h1> \n",
    "<img src=\"text.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{#0077ff}{\\text{Task}}$ \n",
    "\n",
    "\n",
    "* Read all the text from sample_analysis.pdf.\n",
    "* To identify and print the most frequently occurring word.\n",
    "* Extract all the bold words in the document. \n",
    "* Identify and print the frequency of the word ‘Karomi’\n",
    "* Identify the number of tables and the content inside the table.\n",
    "* Represent each word by a number representation (scalar or vector). \n",
    "* Represent words that are similar to each other, by number representations that are closer to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{#80dd00}{\\text{1. Extract Bold words from document}}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import docx library \n",
    "import docx\n",
    "# Read the document \n",
    "df = docx.Document('sample_analysis.docx')\n",
    "# the below for loops checks and print the bold letter from the data\n",
    "for sent in df.paragraphs:\n",
    "    for j in sent.runs:\n",
    "        if j.bold:\n",
    "            print(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the length of the document\n",
    "len(df.paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To print the text using index\n",
    "df.paragraphs[35].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.paragraphs[180].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{#ff0000}{\\text{Docx library}}$ \n",
    "\n",
    "\n",
    "Docx library which has an advantage of preserving document format.\n",
    "other libraries does not preseve the style,colour, font, etc. while reading \n",
    "so to extract bold letters in the text analysis python docx library used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{#00dd00}{\\text{2.The frequency of the word ‘Karomi’}}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Lets check the frequency of the word count using  different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 split command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets extract the text from the dataframe\n",
    "sent = []\n",
    "for n in df.paragraphs: \n",
    "    x = n.text \n",
    "    sent.append(x)\n",
    "\n",
    "df_1 = ' '.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the words using split command\n",
    "df_split = df_1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the lengh of the splitted sentence\n",
    "len(df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below for loop which process over the text to find the word 'karomi'\n",
    "karomi_freq =[]\n",
    "for i in df_split:\n",
    "    if i == 'Karomi':\n",
    "        x = i\n",
    "        # the word 'karomi will append to the above empty list'\n",
    "        karomi_freq.append(x)\n",
    "# to check the frequency of the word.\n",
    "len(karomi_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Frequency  using tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import tokenize module from nltk library\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# tokenize the data\n",
    "df_token=word_tokenize(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the lenght of the words in the document\n",
    "len(df_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The lengths are different from split command and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below for loop which process over the text to find the word 'karomi'\n",
    "karomi_freq =[]\n",
    "for i in df_token:\n",
    "    if i == 'Karomi':\n",
    "        x = i\n",
    "        # the word 'karomi will append to the above empty list'\n",
    "        karomi_freq.append(x)\n",
    "# to check the frequency of the word.\n",
    "len(karomi_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "* The frequency of the word 'Karomi' are different in two algorithms, for split the frequency is 44 and for tokenize the ferquency is 47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check the frequency with other library \"textract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "df_2 = textract.process('sample_analysis.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2= df_2.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets tokenize the data\n",
    "df_token_2=word_tokenize(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the length\n",
    "len(df_token_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below for loop which process over the text to find the word 'karomi'\n",
    "karomi_freq =[]\n",
    "for i in df_token_2:\n",
    "    if i == 'Karomi':\n",
    "        x = i\n",
    "        # the word 'karomi will append to the above empty list'\n",
    "        karomi_freq.append(x)\n",
    "# to check the frequency of the word.\n",
    "len(karomi_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the split command\n",
    "df_split_2 = df_2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_split_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below for loop which process over the text to find the word 'karomi'\n",
    "karomi_freq =[]\n",
    "for i in df_split_2:\n",
    "    if i == 'Karomi':\n",
    "        x = i\n",
    "        # the word 'karomi will append to the above empty list'\n",
    "        karomi_freq.append(x)\n",
    "# to check the frequency of the word.\n",
    "len(karomi_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{#ff0000}{\\text{Textract library}}$ \n",
    "Extract text from any document. No muss. No fuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    " * The frequeny of the word 'Karomi' is 48 \n",
    " * The result from textract library and tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{red}{\\text{3. Identify more frequently occuring word }}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk library\n",
    "import nltk\n",
    "# to find the most \n",
    "most_freq_word = nltk.FreqDist(df_token_2)\n",
    "print('most common positve words',most_freq_word.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_word = nltk.FreqDist(df_split_2)\n",
    "print('most common positve words',most_freq_word.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# to remove stopword\n",
    "stop = set(stopwords.words('french'))\n",
    "# import wordcloud for text visualisation\n",
    "from wordcloud import WordCloud\n",
    "# matlplotlib is a library for 2d visualization\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud(background_color='black',\n",
    "                      stopwords=stop,max_words=100,\n",
    "                      max_font_size=40,scale=3,random_state=1).generate(str(df_token_2))\n",
    "fig = plt.figure(1, figsize=(15, 8))\n",
    "plt.axis('off')\n",
    "plt.imshow(wordcloud)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "* The above plot shows the larger font for text with more frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word vector representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "clean_text = re.sub(r'\\d+', '', df_2)\n",
    "clean_text=clean_text.translate(str.maketrans('', '', string.punctuation))\n",
    "df_token_3=word_tokenize(clean_text)\n",
    "clean_text= [w for w in df_token_3 if not w in stop] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import tfidfvectorizer to convert the text to vector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(clean_text)\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_tfidf[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_count = vectorizer.fit_transform(clean_text)\n",
    "X_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_count[50:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Represent words that are similar and close to each other using TSNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embeddings can be find using different algorithms by various frameworks\n",
    "* Bag of Words (BoW)\n",
    "* Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "* Continuous BoW (CBOW) model and SkipGram model embedding(SkipGram)\n",
    "* Pre-trained word embedding models : \n",
    "     * Word2Vec (by Google)\n",
    "     * GloVe (by Stanford)\n",
    "     * fastText (by Facebook) etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1,2))\n",
    "tf_idf = tf_idf_vect.fit_transform(clean_text)\n",
    "# Standardization to scale data\n",
    "std = StandardScaler(with_mean = False)\n",
    "std_data = std.fit_transform(tf_idf)\n",
    "# Converting sparse matrix to dense because tnse takes only dense vector\n",
    "std_data = std_data.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSNE(n_components = 2, perplexity = 30)\n",
    "tsne_data = model.fit_transform(std_data)\n",
    "tsne_data = np.vstack((tsne_data.T)).T\n",
    "tsne_df = pd.DataFrame(data = tsne_data, columns = (\"X\", \"Y\"))\n",
    "sns.FacetGrid(tsne_df,  height = 8).map(plt.scatter, \"X\", \"Y\").add_legend()\n",
    "plt.title(\"TSNE for TF-IDF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Observation \n",
    "* The above graph shows the simillar and words close to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Extract table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabula library to extract table data\n",
    "from tabula import read_pdf\n",
    "import tabulate\n",
    "df_1 = read_pdf('sample_analysis.pdf',pages='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['Item code 2058051'].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation \n",
    "* The pdf contains three table\n",
    "* One table is an empty table\n",
    "* The other two tables contains data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
